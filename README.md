# Directory Digger

ウェブサイトのディレクトリ構造を探索し、CSVファイルにまとめるデスクトップアプリケーションです。指定したウェブサイトのページを巡回し、ページ情報、外部リンク、リンク切れなどの情報を収集します。

## 機能

- ウェブサイトのページを再帰的に探索
- ページ情報（URL、タイトル、キーワード、ディスクリプション）を収集
- 外部リンクとリンク切れを検出
- 結果をCSVファイルにエクスポート
- マルチスレッドで高速処理
- わかりやすいGUIインターフェース
- リクエスト間の遅延や最大ページ数などの設定が可能

## インストール方法

### 実行可能ファイル（EXE）を使用する方法

Windowsユーザーは、[リリースページ](https://github.com/uriu1709/directory_digger/releases)から最新の`DirectoryDigger.exe`をダウンロードして直接実行できます。インストール不要で使用できます。

### Pythonから実行する方法

#### 前提条件

- Python 3.7以上
- pip (Pythonパッケージマネージャー)

#### セットアップ手順

1. リポジトリをクローンするか、ZIPファイルとしてダウンロードします
   ```
   git clone https://github.com/yourusername/directory_digger.git
   cd directory_digger
   ```

2. 必要なパッケージをインストールします
   ```
   pip install -r requirements.txt
   ```

## 使用方法

### アプリケーションの起動

EXEファイルを使用する場合は、ダウンロードした`DirectoryDigger.exe`をダブルクリックして実行します。

Pythonから実行する場合は以下のコマンドを使用します：
```
python src/main.py
```

### 基本的な使い方

1. 「開始URL」に探索を開始したいウェブサイトのURLを入力します
2. 必要に応じてクロール設定を調整します
   - 最大ページ数: 処理するページ数の上限（0=無制限）
   - 遅延: リクエスト間の待機時間（秒）
   - ワーカー数: 並行して実行するスレッド数
3. 「クロール開始」ボタンをクリックして探索を開始します
4. 探索が完了したら「CSVエクスポート」ボタンをクリックして結果を保存します

## エクスポートされるCSVファイル

1. **ページ情報CSV**: 探索したページの詳細情報
   - URL
   - ページタイトル
   - キーワード
   - ディスクリプション
   - 備考欄

2. **外部リンクCSV**: サイト外部へのリンク情報
   - リンク先URL
   - リンク元ページURL

3. **リンク切れCSV**: 接続できないリンクの情報
   - リンク切れURL
   - リンク元ページURL

## 開発情報

### プロジェクト構造

```
directory_digger/
├── docs/               # ドキュメント
├── resources/          # リソースファイル
├── src/                # ソースコード
│   ├── crawler/        # クローリング機能
│   ├── gui/            # GUIコンポーネント
│   ├── utils/          # ユーティリティ関数
│   └── main.py         # アプリケーションエントリーポイント
├── tests/              # テストコード
├── requirements.txt    # 依存パッケージ
└── README.md           # このファイル
```

### テストの実行

```
python -m unittest discover -s tests
```

## トラブルシューティング

- **クロール中にエラーが発生する場合**: サーバーへのリクエスト頻度が高すぎる可能性があります。「遅延」の値を大きくしてみてください。
- **メモリ使用量が多い場合**: 「最大ページ数」を制限するか、「ワーカー数」を減らしてみてください。

## ライセンス

MIT

## 貢献

バグ報告や機能リクエストは、Issueを作成してください。プルリクエストも歓迎します。